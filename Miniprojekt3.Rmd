---
title: "Miniprojekt 3 - Rasmus"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(optimsimplex)
library(microbenchmark)
library(numDeriv)
```

# Oplæg

## Quasi-Newton


Iterationer givet ved $x_{k+1} = x_k + \alpha_k p_k$ hvor $p_k$ er fundet ved $B_k p_k = - \nabla f_k$, hvis $B_k$ er en approksimation af hessianen, så er det quasi-Newton metoden. Ved quasi-Newton skal kun gradienten anvendes og derfor kan dette være en fordel ift. Newton.

Sekant ligningen som er vigtig for quasi-Newton er at $B_{k+1} s_k = y_k$, hvor $s_k = \alpha_k p_k$ og $y_k = \nabla f_{k+1} - \nabla f_k$. Det man så gør i BFGS er at kigge på den inverse hessian $H_k$ istedet for den almindelige hessian $B_k$. Altså kommer sekant ligningen til at være

\begin{align*}
  H_{k+1} y_k = s_k.
\end{align*}

$H_{k+1}$ skal også være symmetrisk. Dette leder så til at finde en løsning til

\begin{equation}
   \underset{H}{min} ||H - H_k||
\end{equation}
\begin{align}
   &\text{subject to}\\
   &H = H^T, \quad H y_k = s_k.
\end{align}

Som har den unikke løsning

\begin{align*}
  H_{k+1} = (I - \rho_k s_k y_k^T) H_k (I - \rho_k y_k s_k^T) + \rho_k s_k s_k^T, \quad \rho_k := \frac{1}{y_k^T s_k}.
\end{align*}

Som er det BFGS bygger på. Så er spørgsmålet bare hvad man skal vælge $H_0$ til at være, dette har ikke noget klart svar, jeg har valgt $I$, man kunne også finde den rigtige inverse hessian i $x_0$ eller kører BFGS 1 iteration og så vælge den og starte forfra.

Kan ikke sige noget om konvergensen for BFGS fordi det er der ikke nogen der ved.

BFGS er god til at være self-correcting som vist i koden, hvor der tilføjes støj og den så korrigere hurtigt.

## DFO

Hvis ikke $f$'s afledte er tilgængelige, eller man ikke vil bruge forward differencing eller automatic differentiation, så kan man anvende DFO.

Der er mange forskellige typer DFO, fokus på Nelder-Mead.

### Nelder-Mead

For at optimere en objektfunktion $f: \mathbb{R}^n \rightarrow \mathbb{R}$ med Nelder-Mead konstrueres et simplex, bestående af $n+1$ punkter, noteret $\{x_1, x_2, \ldots x_{n+1}\}$ hvor de er ordret sådan at

$$f(x_1) \leq f(x_2) \leq \cdots \leq f(x_{n+1}).$$

Og yderligere noter centroiden som 

$$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i.$$

Og linjen mellem "dårligste" punkt og centroiden som

$$\bar{x}(t) = \bar{x} + t(x_{n+1} - \bar{x}).$$

Så går algoritmen ud på at man vil erstatte det dårligste punkt med et der er bedre ved en af følgenede operationer; reflektering, udvidning, sammentrækning, langs linjen mellem centroiden og det værste punkt. Hvis ikke et bedre punkt kan findes via dette så sammentrækkes simplex'en mod det bedste punkt.

![Nelder-Mead eksempel](C:\Users\rasmu\OneDrive\Dokumenter\Optimering_miniprojekt2\Nelder-Mead_eks.JPG)

# Rosenbrock funktion

```{r "Rosenbrock functions"}
f <- function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
x_min_true <- c(1, 1)
x_0 <- c(-1.2,1)
# analytic derivatives
d_f <- function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))
dd_f <- function(x) rbind(c(1200*x[1]^2 - 400*x[2] + 2, -400*x[1]), c(-400*x[1], 200))
```

# Zoom

```{r "zoom"}
zoom <- function(x_k, a_lo, a_hi, c1, c2, func = f, d_f) {
  f_k <- func(x_k)
  g_k <- d_f(x_k)
  p_k <- -g_k
  
  k <- 0
  k_max <- 1000   # Maximum number of iterations.
  done <- FALSE
  
  while(!done) {
    k <- k + 1
    phi_lo <- func(x_k + a_lo*p_k)
    
    a_k <- 0.5*(a_lo + a_hi)
    phi_k <- func(x_k + a_k*p_k)
    dphi_k_0 <- g_k%*%p_k
    l_k <- f_k + c1*a_k* as.numeric(dphi_k_0)
    
    if ((phi_k > l_k) | (phi_k >= phi_lo)) {
      a_hi <- a_k
    } else {
      dphi_k <- p_k %*% d_f(x_k + a_k*p_k)
      
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
      
      if (dphi_k*(a_hi - a_lo) >= 0) {
        a_hi <- a_lo
      }
      
      a_lo <- a_k
    }
    
    done <- (k > k_max)
  }
  
  return(a_k)
}

alpha <- function(a_0, x_k, c1, c2, func = f, d_f) {
  a_max <- 2*a_0 # Maximum step length. Can also be given as argument.
  f_k <- func(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  
  k <- 0
  k_max <- 1000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- func(x_k)
    g_k <- d_f(x_k)
    p_k <- -g_k
    
    phi_k_old <- phi_k
    phi_k <- func(x_k + a_k*p_k)
    dphi_k_0 <- g_k%*%p_k
    l_k <- f_k + c1*a_k*dphi_k_0
    
    if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
      return(zoom(x_k, a_k_old, a_k, c1, c2, f, d_f))
    }
    
    dphi_k <- p_k %*% d_f(x_k + a_k*p_k)
    
    if (abs(dphi_k) <= -c2*dphi_k_0) {
      return(a_k)
    }
    
    if (dphi_k >= 0) {
      return(zoom(a_k, a_k_old, x_k, c1, c2,f,d_f))
    }
    
    a_k_old <- a_k
    a_k <- rho*a_k + (1 - rho)*a_max # e.g. rho <- 0.5
    done <- (k > k_max)
  }
  
  return(a_k)
}

#params
c1 <- 0.0002
c2 <- 0.9
a_lo <- 0.1
a_hi <- 3
a_0 <- 1
n <- 2
```

# BFGS

```{r "BFGS"}
H_fun <- function(n, H, s_k, y_k) {
  I <- diag(n)
  rho_k <- as.numeric(1/(t(y_k) %*% s_k))
  (I - rho_k * s_k %*% t(y_k)) %*% H %*% (I - rho_k * y_k %*% t(s_k)) + 
    rho_k * s_k %*% t(s_k)
}

BFGS <- function(x_0, fun, dfun, zoom, stoej, output = F) {
  H_k <- diag(n)
  x_k <- x_0
  itt <- 0
  H_diff <- c()
  H_diff_ND <- c()
  while (norm(dfun(x_k),"2") > 1e-5) {
    itt <- itt + 1
    if (output == T) {
      # tjek hvor tæt estimeret H og H er
      H_diff[itt] <- norm(H_k - solve(dd_f(x_k)), "F") 
      H_diff_ND[itt] <- norm(H_k - solve(hessian(f, x_k)), "F")
    }
    p_k <- - H_k %*% dfun(x_k)
    x_k_old <- x_k
    # ifelse(zoom == T, a_k <- zoom(x_k,a_lo,a_hi,c1,c2,f,d_f), a_k <- 1)
    ifelse(zoom == T, a_k <- alpha(a_0, x_k, c1, c2, f, d_f), a_k <- 1)
    x_k <- x_k + a_k * p_k
    s_k <- x_k - x_k_old
    y_k <- dfun(x_k) - dfun(x_k_old)
    H_k <- H_fun(n, H_k, s_k, y_k)
    # Tjek for BFGS evne til at korrigere
    if (itt == 30 & stoej == T) {
      H_k <- H_k + diag(n) * 20 
    }
    if (itt == 100 & stoej == T) {
      H_k[1,2] <- H_k[1,1] + 1000
      H_k <- H_k - diag(n) * 800
    }
  }
  if (output == T) {
    loft <- H_diff < 10000
    #plot af forskel fundet ved numDeriv eller analytisk hessian
    plot(seq_along(H_diff[loft]), y = H_diff[loft])
    plot(seq_along(H_diff_ND[loft]), y = H_diff_ND[loft])
    cat("Frobenius norm forskelle stoerre end 100 =", "\n", H_diff[!loft], "\n")
    cat("x* =", x_k, "\n", "antal iterationer =", itt)
  }
}

BFGS(x_0, f, d_f, zoom = F, stoej = F, output = T)
BFGS(x_0, f, d_f, zoom = F, stoej = T, output = T)


# BFGS(x_0, f, d_f, zoom = T, stoej = F, output = T)
```

# Nelder-Mead

```{r "Nelder-Mead"}
x_bar_fun <- function(x) {
  1/ 2 * colSums(x[-3,])
}
x_bar_t <- function(t, xn1, x_bar) {
  x_bar + t * (xn1 - x_bar)
}

f_order <- function(x,f) {
  resul <- apply(x, 1, f)
  order(resul)
}

Nelder_mead <- function(x_0, f, output = F) {
  x_n <- optimsimplex(method = "spendley", x0 = x_0)$newobj$x
  itt <- 0
  while (abs(f(x_n[3,] - f(x_n[1,]))) > 1e-10) {
    itt <- itt + 1
    x_n_old <- x_n
    order <- f_order(x_n, f)
    x_n <- x_n[order,]
    x_bar <- x_bar_fun(x_n)
    
    x_bar_m1 <- x_bar_t(-1, x_n[3,], x_bar)
    f_x_m1 <- f(x_bar_m1)
    if ( f(x_n[1,]) <= f_x_m1 & f_x_m1 < f(x_n[2,])) {
      x_n[3,] <- x_bar_m1
    }
    else 
      if (f_x_m1 < f(x_n[1,])) {
        x_bar_m2 <-x_bar_t(-2, x_n[3,], x_bar)
        f_x_m2 <- f(x_bar_m2)
        if (f_x_m2 < f_x_m1) {
          x_n[3,] <- x_bar_m2
        }
        else {
          x_n[3,] <- x_bar_m1
        }
      }
    
    else { 
      if (f_x_m1 >= f(x_n[2,])) {
        if (f(x_n[2,]) <= f_x_m1 & f_x_m1 < f(x_n[3,])) {
          x_bar_m12 <- x_bar_t(-0.5, x_n[3,], x_bar)
          f_x_m12 <- f(x_bar_m12)
          if (f_x_m12 <= f_x_m1) {
            x_n[3,] <- x_bar_m12
          }
        }
        else {
          x_bar_p12 <- x_bar_t(0.5, x_n[3,], x_bar)
          f_x_p12 <- f(x_bar_p12)
          if (f_x_p12 < f(x_n[3,])) {
            x_n[3,] <- x_bar_p12
          }
        }
      }
    }
    if (norm(x_n - x_n_old, type = "F") < 0.001) {
      for (i in 2:3) {
        x_n[i,] <- 0.5 * (x_n[1,] + x_n[i,])
      }
    }
  }
  if (output == T) {
    cat("x* = ", x_n[3,], "\n", "f(x*) = ", f(x_n), "\n", "Antal iterationer=", itt)
  }
}

Nelder_mead(x_0, f, output = T)

```

# Gradient descent, identitets matrix

```{r "Gradient descent - steepest descent - identitet"}
optimer_identitet_zoom <- function(x_0, f, d_f, output = F) {
  x_k <- x_0
  a_k <- 2
  itt <- 0
  while (norm(d_f(x_k), type = "2") > 1e-5) {
    itt <- 1 + itt
    p_k <- -d_f(x_k)
    a_k <- alpha(a_0, x_k, c1,c2,f, d_f)
    x_k <- x_k + a_k * t(p_k)
    if (itt > 100000) {
      stop("Too many iterations")
    }
  }
  if (output == T) {
    cat("x* = ", x_k, "\n", "f(x*) =", f(x_k), "\n", "iteration =", itt)
  }
}
optimer_identitet_zoom(x_0, f, d_f, output = T)
```

# Gradient descent, newton

```{r "Gradient descent - steepest descent - Newton"}
optimer_newton_zoom <- function(x_0, f, d_f, dd_f, output = F) {
  x_k <- x_0
  a_k <- 2
  itt <- 0
  while (norm(t(d_f(x_k)), type = "2") > 1e-4) {
    itt <- 1 + itt
    p_k <- solve(dd_f(x_k), -d_f(x_k))
    # a_k <- alpha(a_0, x_k, c1,c2, f, d_f)
    a_k <- 1
    x_k <- x_k + a_k * t(p_k)
    if (itt > 100000) {
      stop("Too many iterations")
    }
  }
  if (output == T) {
    cat("x* = ", x_k, "\n", "f(x*) =", f(x_k), "\n", "iteration =", itt)
  }
}
optimer_newton_zoom(x_0, f, d_f, dd_f, output = T)
```

\newpage

# Benchmark af Rosenbrock funktion

```{r "Benchmarking Rosenbrock function"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F),
               Nelder_mead(x_0, f,output = F),
               optimer_identitet_zoom(x_0, f, d_f),
               optimer_newton_zoom(x_0, f, d_f, dd_f,output = F) , times = 10)
```

# Convex Elliptical funktion

```{r "Convex Elliptical function"}
f <- function(x) 0.5*(100*x[1]^2 + x[2]^2)
f_xy <- function(x,y) 0.5*(100*x^2 + y^2)
x_min_true <- c(0, 0)
x_0 <- c(-1.2,1.3)

# analytic derivatives
d_f <- function(x) c(0.5*200*x[1], 0.5*2*x[2])
dd_f <- function(x) 0.5*rbind(c(200, 0), c(0, 2))
```

\newpage

# Benchmarking Convex Elliptical funktion

```{r "Benchmarking Convex Elliptical function"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F),
               Nelder_mead(x_0, f),
               optimer_identitet_zoom(x_0, f, d_f),
               optimer_newton_zoom(x_0, f, d_f, dd_f,output = F) , times = 10)
```

# Nonconvex sines funktion

```{r "Nonconvex sines function" }
f <- function(x) x[1]^2 + .25*x[2]^2 + 4*(x[1] - x[2])^2*sin(x[2])^2
x_min_true <- c(0, 0)
x_0 <- c(-1,1)

# analytic derivatives
d_f <- function(x) c(2*x[1] + 8*(x[1] - x[2])*sin(x[2])^2, .5*x[2] - 8*(x[1] - x[2])*sin(x[2])^2 + 4*(x[1] - x[2])^2*sin(2*x[2]))
dd_f <- function(x) {
  H11 <- 2 + 8*sin(x[2])^2
  H12 <- 8*(2*(x[1] - x[2])*cos(x[2]) - sin(x[2]))*sin(x[2])
  H21 <- H12
  H22 <- 4.5 + 4*(2*(x[1] - x[2])^2 - 1)*cos(2*x[2]) + 16*(x[2] - x[1])*sin(2*x[2])
  rbind(c(H11, H12), c(H21, H22))
}
```

\newpage

# Benchmarking Nonconvex sines funktion

```{r "Benchmarking"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F),
               Nelder_mead(x_0, f),
               optimer_newton_zoom(x_0, f, d_f, dd_f, output = F) , times = 10)
```

